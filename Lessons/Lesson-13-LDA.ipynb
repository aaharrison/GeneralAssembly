{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "+ Most commonly used in natural language processing\n",
    "+ Sometimes as an end in and of itself\n",
    "+ Sometimes as a variable reduction technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example of LDA in NLP\n",
    "\n",
    "Stolen from: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "+ Authors: \n",
    "    + Olivier Grisel <olivier.grisel@ensta.org>\n",
    "    + Lars Buitinck\n",
    "    + Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "+ License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "os.chdir(\"/Users/adeniyiharrison/Desktop/General Assembly/DS-SF-32/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code defines a custom function that we'll use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code loads the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2.504s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have a Roberto Clemente 1969 Topps baseball card for sale, in near-mint\\ncondition (really as close to mint condition as you can get).  It lists for\\n$55 in my most recent baseball card pricelist for May.  I am offering it for\\n$50 and I'll pay the certified postage to ship it to you.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"data\"][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.412s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "# Max df and min df trottles words, say a super common word comes up too frequently then drop it, if it never comes up drop it\n",
    "# If below 1 then its a percentage if its above 1 then its a specific count\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "# n_samples = 2000\n",
    "# n_features = 1000\n",
    "# n_topics = 10\n",
    "# n_top_words = 20\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics = n_topics, max_iter = 5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.601s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>applications</th>\n",
       "      <th>apply</th>\n",
       "      <th>appreciated</th>\n",
       "      <th>approach</th>\n",
       "      <th>appropriate</th>\n",
       "      <th>apr</th>\n",
       "      <th>april</th>\n",
       "      <th>archive</th>\n",
       "      <th>area</th>\n",
       "      <th>areas</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>xfree86</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 900 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   applications  apply  appreciated  approach  appropriate  apr  april  \\\n",
       "0             0      0            0         0            0    0      0   \n",
       "1             0      0            0         0            0    0      0   \n",
       "2             0      0            0         0            0    0      0   \n",
       "3             0      0            0         0            0    0      0   \n",
       "4             0      0            0         0            0    0      0   \n",
       "\n",
       "   archive  area  areas  ...    worth  wouldn  write  written  wrong  xfree86  \\\n",
       "0        0     0      0  ...        0       0      0        0      0        0   \n",
       "1        0     0      0  ...        0       0      0        0      0        0   \n",
       "2        0     0      0  ...        0       0      0        0      0        0   \n",
       "3        0     1      0  ...        0       0      0        0      0        0   \n",
       "4        0     0      0  ...        0       0      0        0      0        0   \n",
       "\n",
       "   year  years  yes  young  \n",
       "0     0      0    0      0  \n",
       "1     0      0    0      0  \n",
       "2     0      0    0      0  \n",
       "3     1      0    1      0  \n",
       "4     0      0    0      0  \n",
       "\n",
       "[5 rows x 900 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf.A, columns = [tf_vectorizer.get_feature_names()]).iloc[:,100:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1:\n",
      "don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2:\n",
      "christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3:\n",
      "drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4:\n",
      "hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5:\n",
      "god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6:\n",
      "55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7:\n",
      "car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8:\n",
      "people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9:\n",
      "key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class assignment\n",
    "\n",
    "+ load in the training set (done for you below)\n",
    "+ re-run LDA and use topics as input for model\n",
    "+ Predict categories using some multinomial classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2.222s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'), \n",
    "                            subset=\"train\")\n",
    "\n",
    "data = dataset.data\n",
    "\n",
    "y = dataset.target\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    600\n",
       "15    599\n",
       "8     598\n",
       "9     597\n",
       "11    595\n",
       "13    594\n",
       "7     594\n",
       "14    593\n",
       "5     593\n",
       "12    591\n",
       "2     591\n",
       "3     590\n",
       "6     585\n",
       "1     584\n",
       "4     578\n",
       "17    564\n",
       "16    546\n",
       "0     480\n",
       "18    465\n",
       "19    377\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to continue this think tank charade of yours, your\\nfixation on Israel must stop.  You might have to start asking the\\nsame sort of questions of Arab countries as well.  You realize it\\nwould not work, as the Arab countries' treatment of Jews over the\\nlast several decades is so bad that your fixation on Israel would\\nbegin to look like the biased attack that it is.\\n\\n   Everyone in this group recognizes that your stupid 'Center for\\nPolicy Research' is nothing more than a fancy name for some bigot\\nwho hates Israel.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "countVect = CountVectorizer(stop_words = \"english\", \n",
    "                            max_df = .90, \n",
    "                            min_df = 10)\n",
    "\n",
    "X = countVect.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 10441)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ax</th>\n",
       "      <td>62387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>3964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don</th>\n",
       "      <td>3885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>3752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>3487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>3011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>2968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "ax      62387\n",
       "max      4585\n",
       "people   4103\n",
       "like     3964\n",
       "don      3885\n",
       "just     3752\n",
       "know     3487\n",
       "use      3179\n",
       "think    3011\n",
       "time     2968"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    np.sum(pd.DataFrame(X.A , columns = [countVect.get_feature_names()]).iloc[:,800:])\n",
    ").sort_values(0, ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "year game don just think team good like time know ll games got going didn did season better play players\n",
      "Topic #1:\n",
      "car bike cars water engine air dod miles road oil vehicle ride radar auto gas speed ground high hot riding\n",
      "Topic #2:\n",
      "key government public president encryption use security chip keys clipper information technology law privacy national private administration new number data\n",
      "Topic #3:\n",
      "edu com mail ftp email send cs graphics file available list dos version thanks pub address files pc windows ca\n",
      "Topic #4:\n",
      "people said armenian israel armenians war turkish jews years killed children israeli russian government did time women turkey food went\n",
      "Topic #5:\n",
      "10 00 15 25 20 11 12 14 16 17 13 18 30 24 19 50 21 23 22 27\n",
      "Topic #6:\n",
      "people god don think just say does know believe like jesus time make way right did good question point things\n",
      "Topic #7:\n",
      "like use know just don does problem drive good time used work ve need new want thanks bit using make\n",
      "Topic #8:\n",
      "ax max b8f g9v a86 pl 145 1d9 0t 34u 1t 3t giz bhj wm 75u 2tm cx bxn 7ey\n",
      "Topic #9:\n",
      "file program space window use available data entry information output image code server set nasa files using source motif application\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics = n_topics, max_iter = 5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(X)\n",
    "\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = countVect.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.331789749371\n"
     ]
    }
   ],
   "source": [
    "X = lda.transform(X)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 10, weights = \"distance\")\n",
    "print(\"Accuracy: \", cross_val_score(knn, X, y, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0018873 ,  0.0018869 ,  0.00188689, ...,  0.00188711,\n",
       "         0.00188679,  0.00188689],\n",
       "       [ 0.25293131,  0.02497489,  0.00212803, ...,  0.09812988,\n",
       "         0.00212766,  0.00212806],\n",
       "       [ 0.00217472,  0.00217439,  0.00217423, ...,  0.00217452,\n",
       "         0.00217391,  0.00217424],\n",
       "       ..., \n",
       "       [ 0.89999326,  0.01111195,  0.01111132, ...,  0.01111266,\n",
       "         0.01111111,  0.01111134],\n",
       "       [ 0.00555642,  0.00555589,  0.00555628, ...,  0.76227748,\n",
       "         0.00555606,  0.00555625],\n",
       "       [ 0.68316872,  0.00105296,  0.00105351, ...,  0.24421006,\n",
       "         0.00105263,  0.00105312]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator = RandomForestClassifier(),\n",
    "            param_grid = {\"n_estimators\" : np.arange(10,21,1).tolist()},\n",
    "            cv = KFold(n_splits = 5))\n",
    "\n",
    "gs.fit(X,y)\n",
    "algo = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96906487537564079"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[466,   0,   0,   0,   0,   0,   0,  12,   0,   0,   0,   0,   0,\n",
       "          0,   0,   1,   0,   0,   0,   1],\n",
       "       [  0, 565,   1,   0,   0,   1,   0,  16,   0,   0,   0,   0,   0,\n",
       "          1,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   2, 561,   0,   0,   1,   0,  27,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   1, 572,   1,   0,   1,  13,   0,   0,   0,   0,   1,\n",
       "          0,   0,   0,   0,   0,   1,   0],\n",
       "       [  0,   0,   0,   1, 555,   0,   0,  22,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 590,   0,   3,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   1,   0,   0,   2,   0, 572,   8,   0,   0,   0,   0,   2,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   1,   0,   0,   0,   0,   0, 592,   0,   0,   0,   0,   1,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   1,   0,   0,   1,   0,   0,  15, 581,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  30,   0, 566,   1,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  17,   0,   2, 581,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  2,   0,   0,   0,   0,   0,   0,  13,   0,   0,   0, 579,   0,\n",
       "          0,   0,   0,   0,   0,   1,   0],\n",
       "       [  0,   0,   0,   0,   2,   0,   1,  17,   0,   0,   0,   0, 571,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  17,   0,   0,   0,   0,   0,\n",
       "        576,   0,   0,   0,   1,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  16,   1,   0,   0,   1,   0,\n",
       "          1, 574,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   9,   0,   0,   0,   0,   0,\n",
       "          0,   0, 590,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  14,   0,   0,   0,   0,   0,\n",
       "          0,   0,   1, 530,   1,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  19,   0,   1,   0,   0,   0,\n",
       "          1,   0,   0,   0, 543,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  16,   0,   1,   0,   2,   0,\n",
       "          0,   0,   0,   0,   0, 446,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  19,   0,   0,   0,   0,   1,\n",
       "          0,   0,   2,   0,   1,   0, 354]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y, algo.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-7f7891c5ee6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "algo = lda.fit(X)\n",
    "X_test = countVect.transform(data)\n",
    "X_test = algo.transform(X_test)\n",
    "\n",
    "pred = algo.predict(X_test)\n",
    "metrics.f1_score(dataset.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class assignment:\n",
    "\n",
    "+ I'll divide you into 3 segments\n",
    "+ Each segment generates 100 sentences on the *same topic*\n",
    "+ Save as a JSON and send to me\n",
    "+ We'll run them through LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASSLIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = [\"My favorite type of food is tacos, but it used to be fried chicken.\", \"My favorite type of taco is al pastor.\", \"My favorite mexican resturant is El Rancho.\",\n",
    "            \"I also like all the resturants in my immediate neighborhood.\", \"Corn dogs are quite nice as well however my friends make fun of me\",\n",
    "            \"Yo dog I love dog, but not like the food\", \"Sup hot stuff, you like hot food or cold food\", \"I raise chickens in my farm that I dont eat\",\n",
    "            \"Sometimes I go fishing with my father\", \"Working at the food bank is very fulfilling to me\", \n",
    "            \"If I eat too much Im not going to feel like drinking\", \"The breweries in San Diego are plentiful\", \n",
    "            \"The food in San Deigo are not as good as the stuff in SF\", \"LA has really solid mexican food which I love\",\n",
    "            \"Im pretty hungry right now, where should we grab lunch?\", \"Is dinner going to be taken care of at the Reynold's house?\",\n",
    "            \"If I pay for breakfast will you cover lunch or dinner babe?\"]\n",
    "\n",
    "s2 = [\"Fast Food is not good for health\", \"Indian food is spicy\", \"I like Thai food\", \"There are two new restaurants opened around the block\", \n",
    "      \"Can I get this sweet dish?\", \"McBurger has 3000 calories\", \"Nuts are good for health\", \"Vegetables are bad\", \n",
    "      \"Cheese cake is good\", \"He ate all the fried food\", \"In istanbul, a burger cost $30\", \"The new hotel chain offers free buffet for 2 days\",\n",
    "      \"Can I get a diet coke?\", \"How to cook fiesta salsa?\", \"These fries are tasty but bad for health\", \n",
    "      \"This chinese restaurant serves the best soup\", \"Please order a pizza for me?\", \"Dinner is ready\", \n",
    "      \"Doing breakfast is good for health\", \"Please dont throw extra food, donate it to someone hungry\"]\n",
    "\n",
    "s3 = [\"chocolate chip cookies and best fresh from the oven.\", \"pumpkin pie is a good dessert for the fall season\", \"vegtables are an important part of any diet\", \"fruit is a healthy way to suffice your sweet tooth\", \"eggs are a filling way eat breakfast\", \"soda is a necessary evil.\", \"philz coffee is a great way to start your morning\", \"after making a big dinner with several courses, at least there are leftovers.\", \"turnkey is a great type of meat\", \"hot sauce makes everything better.\", \"hot dogs and garlic fries are best when watching a giants baseball game.\", \"I like ketchup more than mustard\", \"I wish a had a few more cook books.\", \"The worst part of cooking is cleaning the pots and pans afterwards.\", \"I had cereal with a banana every morning before school as a kid.\", \"Avocado is my favorite type of vegtable.\", \"I try to avoid fast food restaurants as much as possible.\", \"shrimp scampi is one of my all time favorite dishes.\", \"cooking is something I hope to do more of later in life.\", \"salmon is a great type of food\"]\n",
    "\n",
    "s4 = [\"You should eat well, but not like Charles Barkley well.\",\n",
    "\"There are like 17 cooking shows. All of them seem to be related to Top Chef.\",\n",
    "\"Guy Fearri is not a chef so much as the lead from Smashmouth pretending to be a chef.\",\n",
    "\"Salt is not a food. But it goes well on food.\",\n",
    "\"Vegetarians who still eat fish are not vegetarians. They are just against eating things that have eyes.\",\n",
    "\"Vegans are basically food Taliban. Do not make me feel bad because I have good things in my life.\",\n",
    "\"They say cows shitting causes global warming. That means we should eat less cows. Maybe more veal though. What is the shit to meat produced ratio where we can still enjoy meat, but not destroy the only planet we have.\",\n",
    "\"My mother said pre-heat the oven. Instead I turned on the microwave.\",\n",
    "\"Turkey is the worst of the bird dishes.\",\n",
    "\"Dog is a food someplaces.\",\n",
    "\"To make rice, you just get rice, and then add water.\",\n",
    "\"Food Trucks are not made of food.\",\n",
    "\"Instagram is mostly a forum for posting food photos. ALso for Smirnoff ICe ads.\",\n",
    "\"Pasta is a delicacy.\",\n",
    "\"I refused to believe that gushers are a food.\",\n",
    "\"If you travel exclusively for local dishes, you have too much money.\",\n",
    "\"Happiness: a good bank account, a good cook, and a good digestion.\",\n",
    "\"Food Porn and Porn Food are not the same thing, and you should google only one.\",\n",
    "\"France thinks it has the best food in Europe, but really Italy does. In Asia, Thailand is to France, as Vietnam is to Italy. I will not negotiate on this.\"]\n",
    "\n",
    "s5 = [\"My favorite food is a delicious cheeseburger.\", \"Common toppings on cheeseburgers include mayo, ketchup, pickles, grilled onion, lettuce, and aged cheddar.\", \"If I had to choose my favorite cuisine, it would be Italian.\", \"I love Philly Cheesesteak Sandwiches, gotta have those grilled onions, White American cheese, hot and sweet peppers on a hoagie.\", \"Despite all this, I also try to eat vegetarian a few days a week.\", \"Need a cheap week night meal?\", \"How about a baked potato with all the fixins'!\", \"We're talking about sour cream, green onion, salt, pepper, cheese, and bacon.\", \"Cooking can really help one save money, going out to eat gets expensive and adds up quickly over time.\", \"The easiest meals to cook for me are breakfast.\", \"Been a breakfast lover since day one, pancakes, bacon, sausage, eggs any way ya like em', the whole nine yards.\", \"Breakfast is not the most important meal of the day, it was a marketing scam from the early 20th century.\", \"Talk about a conspiracy, Kellog's is at the bottom of this one.\", \"Right before class I ate at the Halal Guys.\", \"They serve up delicious combo platter featuring gyro style beef, chicken, falafel, and a variety of sauces.\", \"Watch out for the hot sauce, my god!\", \"It is one of the hottest things I've ever eaten, and they put it in ketchup bottles.\", \"Many a drunken fool has accidentally lit his mouth on fire with that sauce.\", \"Another great breakfast...a whiskey ginger.\", \"Only drink those on the weekends though.\", \"The people who cook garlic fries are doing God's work.\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My favorite type of food is tacos, but it used to be fried chicken.',\n",
       " 'My favorite type of taco is al pastor.',\n",
       " 'My favorite mexican resturant is El Rancho.',\n",
       " 'I also like all the resturants in my immediate neighborhood.',\n",
       " 'Corn dogs are quite nice as well however my friends make fun of me',\n",
       " 'Yo dog I love dog, but not like the food',\n",
       " 'Sup hot stuff, you like hot food or cold food',\n",
       " 'I raise chickens in my farm that I dont eat',\n",
       " 'Sometimes I go fishing with my father',\n",
       " 'Working at the food bank is very fulfilling to me',\n",
       " 'If I eat too much Im not going to feel like drinking',\n",
       " 'The breweries in San Diego are plentiful',\n",
       " 'The food in San Deigo are not as good as the stuff in SF',\n",
       " 'LA has really solid mexican food which I love',\n",
       " 'Im pretty hungry right now, where should we grab lunch?',\n",
       " \"Is dinner going to be taken care of at the Reynold's house?\",\n",
       " 'If I pay for breakfast will you cover lunch or dinner babe?',\n",
       " 'Fast Food is not good for health',\n",
       " 'Indian food is spicy',\n",
       " 'I like Thai food',\n",
       " 'There are two new restaurants opened around the block',\n",
       " 'Can I get this sweet dish?',\n",
       " 'McBurger has 3000 calories',\n",
       " 'Nuts are good for health',\n",
       " 'Vegetables are bad',\n",
       " 'Cheese cake is good',\n",
       " 'He ate all the fried food',\n",
       " 'In istanbul, a burger cost $30',\n",
       " 'The new hotel chain offers free buffet for 2 days',\n",
       " 'Can I get a diet coke?',\n",
       " 'How to cook fiesta salsa?',\n",
       " 'These fries are tasty but bad for health',\n",
       " 'This chinese restaurant serves the best soup',\n",
       " 'Please order a pizza for me?',\n",
       " 'Dinner is ready',\n",
       " 'Doing breakfast is good for health',\n",
       " 'Please dont throw extra food, donate it to someone hungry',\n",
       " 'chocolate chip cookies and best fresh from the oven.',\n",
       " 'pumpkin pie is a good dessert for the fall season',\n",
       " 'vegtables are an important part of any diet',\n",
       " 'fruit is a healthy way to suffice your sweet tooth',\n",
       " 'eggs are a filling way eat breakfast',\n",
       " 'soda is a necessary evil.',\n",
       " 'philz coffee is a great way to start your morning',\n",
       " 'after making a big dinner with several courses, at least there are leftovers.',\n",
       " 'turnkey is a great type of meat',\n",
       " 'hot sauce makes everything better.',\n",
       " 'hot dogs and garlic fries are best when watching a giants baseball game.',\n",
       " 'I like ketchup more than mustard',\n",
       " 'I wish a had a few more cook books.',\n",
       " 'The worst part of cooking is cleaning the pots and pans afterwards.',\n",
       " 'I had cereal with a banana every morning before school as a kid.',\n",
       " 'Avocado is my favorite type of vegtable.',\n",
       " 'I try to avoid fast food restaurants as much as possible.',\n",
       " 'shrimp scampi is one of my all time favorite dishes.',\n",
       " 'cooking is something I hope to do more of later in life.',\n",
       " 'salmon is a great type of food',\n",
       " 'You should eat well, but not like Charles Barkley well.',\n",
       " 'There are like 17 cooking shows. All of them seem to be related to Top Chef.',\n",
       " 'Guy Fearri is not a chef so much as the lead from Smashmouth pretending to be a chef.',\n",
       " 'Salt is not a food. But it goes well on food.',\n",
       " 'Vegetarians who still eat fish are not vegetarians. They are just against eating things that have eyes.',\n",
       " 'Vegans are basically food Taliban. Do not make me feel bad because I have good things in my life.',\n",
       " 'They say cows shitting causes global warming. That means we should eat less cows. Maybe more veal though. What is the shit to meat produced ratio where we can still enjoy meat, but not destroy the only planet we have.',\n",
       " 'My mother said pre-heat the oven. Instead I turned on the microwave.',\n",
       " 'Turkey is the worst of the bird dishes.',\n",
       " 'Dog is a food someplaces.',\n",
       " 'To make rice, you just get rice, and then add water.',\n",
       " 'Food Trucks are not made of food.',\n",
       " 'Instagram is mostly a forum for posting food photos. ALso for Smirnoff ICe ads.',\n",
       " 'Pasta is a delicacy.',\n",
       " 'I refused to believe that gushers are a food.',\n",
       " 'If you travel exclusively for local dishes, you have too much money.',\n",
       " 'Happiness: a good bank account, a good cook, and a good digestion.',\n",
       " 'Food Porn and Porn Food are not the same thing, and you should google only one.',\n",
       " 'France thinks it has the best food in Europe, but really Italy does. In Asia, Thailand is to France, as Vietnam is to Italy. I will not negotiate on this.',\n",
       " 'My favorite food is a delicious cheeseburger.',\n",
       " 'Common toppings on cheeseburgers include mayo, ketchup, pickles, grilled onion, lettuce, and aged cheddar.',\n",
       " 'If I had to choose my favorite cuisine, it would be Italian.',\n",
       " 'I love Philly Cheesesteak Sandwiches, gotta have those grilled onions, White American cheese, hot and sweet peppers on a hoagie.',\n",
       " 'Despite all this, I also try to eat vegetarian a few days a week.',\n",
       " 'Need a cheap week night meal?',\n",
       " \"How about a baked potato with all the fixins'!\",\n",
       " \"We're talking about sour cream, green onion, salt, pepper, cheese, and bacon.\",\n",
       " 'Cooking can really help one save money, going out to eat gets expensive and adds up quickly over time.',\n",
       " 'The easiest meals to cook for me are breakfast.',\n",
       " \"Been a breakfast lover since day one, pancakes, bacon, sausage, eggs any way ya like em', the whole nine yards.\",\n",
       " 'Breakfast is not the most important meal of the day, it was a marketing scam from the early 20th century.',\n",
       " \"Talk about a conspiracy, Kellog's is at the bottom of this one.\",\n",
       " 'Right before class I ate at the Halal Guys.',\n",
       " 'They serve up delicious combo platter featuring gyro style beef, chicken, falafel, and a variety of sauces.',\n",
       " 'Watch out for the hot sauce, my god!',\n",
       " \"It is one of the hottest things I've ever eaten, and they put it in ketchup bottles.\",\n",
       " 'Many a drunken fool has accidentally lit his mouth on fire with that sauce.',\n",
       " 'Another great breakfast...a whiskey ginger.',\n",
       " 'Only drink those on the weekends though.',\n",
       " \"The people who cook garlic fries are doing God's work.\"]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = s1+s2+s3+s4+s5\n",
    "\n",
    "sentences = {\"Food\": sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/adeniyiharrison/Desktop\")\n",
    "import json\n",
    "with open(\"Food Sentences.json\", \"w\") as x:\n",
    "    json.dump(sentences, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Food': ['My favorite type of food is tacos, but it used to be fried chicken.',\n",
       "  'My favorite type of taco is al pastor.',\n",
       "  'My favorite mexican resturant is El Rancho.',\n",
       "  'I also like all the resturants in my immediate neighborhood.',\n",
       "  'Corn dogs are quite nice as well however my friends make fun of me',\n",
       "  'Yo dog I love dog, but not like the food',\n",
       "  'Sup hot stuff, you like hot food or cold food',\n",
       "  'I raise chickens in my farm that I dont eat',\n",
       "  'Sometimes I go fishing with my father',\n",
       "  'Working at the food bank is very fulfilling to me',\n",
       "  'If I eat too much Im not going to feel like drinking',\n",
       "  'The breweries in San Diego are plentiful',\n",
       "  'The food in San Deigo are not as good as the stuff in SF',\n",
       "  'LA has really solid mexican food which I love',\n",
       "  'Im pretty hungry right now, where should we grab lunch?',\n",
       "  \"Is dinner going to be taken care of at the Reynold's house?\",\n",
       "  'If I pay for breakfast will you cover lunch or dinner babe?',\n",
       "  'Fast Food is not good for health',\n",
       "  'Indian food is spicy',\n",
       "  'I like Thai food',\n",
       "  'There are two new restaurants opened around the block',\n",
       "  'Can I get this sweet dish?',\n",
       "  'McBurger has 3000 calories',\n",
       "  'Nuts are good for health',\n",
       "  'Vegetables are bad',\n",
       "  'Cheese cake is good',\n",
       "  'He ate all the fried food',\n",
       "  'In istanbul, a burger cost $30',\n",
       "  'The new hotel chain offers free buffet for 2 days',\n",
       "  'Can I get a diet coke?',\n",
       "  'How to cook fiesta salsa?',\n",
       "  'These fries are tasty but bad for health',\n",
       "  'This chinese restaurant serves the best soup',\n",
       "  'Please order a pizza for me?',\n",
       "  'Dinner is ready',\n",
       "  'Doing breakfast is good for health',\n",
       "  'Please dont throw extra food, donate it to someone hungry',\n",
       "  'chocolate chip cookies and best fresh from the oven.',\n",
       "  'pumpkin pie is a good dessert for the fall season',\n",
       "  'vegtables are an important part of any diet',\n",
       "  'fruit is a healthy way to suffice your sweet tooth',\n",
       "  'eggs are a filling way eat breakfast',\n",
       "  'soda is a necessary evil.',\n",
       "  'philz coffee is a great way to start your morning',\n",
       "  'after making a big dinner with several courses, at least there are leftovers.',\n",
       "  'turnkey is a great type of meat',\n",
       "  'hot sauce makes everything better.',\n",
       "  'hot dogs and garlic fries are best when watching a giants baseball game.',\n",
       "  'I like ketchup more than mustard',\n",
       "  'I wish a had a few more cook books.',\n",
       "  'The worst part of cooking is cleaning the pots and pans afterwards.',\n",
       "  'I had cereal with a banana every morning before school as a kid.',\n",
       "  'Avocado is my favorite type of vegtable.',\n",
       "  'I try to avoid fast food restaurants as much as possible.',\n",
       "  'shrimp scampi is one of my all time favorite dishes.',\n",
       "  'cooking is something I hope to do more of later in life.',\n",
       "  'salmon is a great type of food',\n",
       "  'You should eat well, but not like Charles Barkley well.',\n",
       "  'There are like 17 cooking shows. All of them seem to be related to Top Chef.',\n",
       "  'Guy Fearri is not a chef so much as the lead from Smashmouth pretending to be a chef.',\n",
       "  'Salt is not a food. But it goes well on food.',\n",
       "  'Vegetarians who still eat fish are not vegetarians. They are just against eating things that have eyes.',\n",
       "  'Vegans are basically food Taliban. Do not make me feel bad because I have good things in my life.',\n",
       "  'They say cows shitting causes global warming. That means we should eat less cows. Maybe more veal though. What is the shit to meat produced ratio where we can still enjoy meat, but not destroy the only planet we have.',\n",
       "  'My mother said pre-heat the oven. Instead I turned on the microwave.',\n",
       "  'Turkey is the worst of the bird dishes.',\n",
       "  'Dog is a food someplaces.',\n",
       "  'To make rice, you just get rice, and then add water.',\n",
       "  'Food Trucks are not made of food.',\n",
       "  'Instagram is mostly a forum for posting food photos. ALso for Smirnoff ICe ads.',\n",
       "  'Pasta is a delicacy.',\n",
       "  'I refused to believe that gushers are a food.',\n",
       "  'If you travel exclusively for local dishes, you have too much money.',\n",
       "  'Happiness: a good bank account, a good cook, and a good digestion.',\n",
       "  'Food Porn and Porn Food are not the same thing, and you should google only one.',\n",
       "  'France thinks it has the best food in Europe, but really Italy does. In Asia, Thailand is to France, as Vietnam is to Italy. I will not negotiate on this.',\n",
       "  'My favorite food is a delicious cheeseburger.',\n",
       "  'Common toppings on cheeseburgers include mayo, ketchup, pickles, grilled onion, lettuce, and aged cheddar.',\n",
       "  'If I had to choose my favorite cuisine, it would be Italian.',\n",
       "  'I love Philly Cheesesteak Sandwiches, gotta have those grilled onions, White American cheese, hot and sweet peppers on a hoagie.',\n",
       "  'Despite all this, I also try to eat vegetarian a few days a week.',\n",
       "  'Need a cheap week night meal?',\n",
       "  \"How about a baked potato with all the fixins'!\",\n",
       "  \"We're talking about sour cream, green onion, salt, pepper, cheese, and bacon.\",\n",
       "  'Cooking can really help one save money, going out to eat gets expensive and adds up quickly over time.',\n",
       "  'The easiest meals to cook for me are breakfast.',\n",
       "  \"Been a breakfast lover since day one, pancakes, bacon, sausage, eggs any way ya like em', the whole nine yards.\",\n",
       "  'Breakfast is not the most important meal of the day, it was a marketing scam from the early 20th century.',\n",
       "  \"Talk about a conspiracy, Kellog's is at the bottom of this one.\",\n",
       "  'Right before class I ate at the Halal Guys.',\n",
       "  'They serve up delicious combo platter featuring gyro style beef, chicken, falafel, and a variety of sauces.',\n",
       "  'Watch out for the hot sauce, my god!',\n",
       "  \"It is one of the hottest things I've ever eaten, and they put it in ketchup bottles.\",\n",
       "  'Many a drunken fool has accidentally lit his mouth on fire with that sauce.',\n",
       "  'Another great breakfast...a whiskey ginger.',\n",
       "  'Only drink those on the weekends though.',\n",
       "  \"The people who cook garlic fries are doing God's work.\"]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
